<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>The MASSIF project - Work packages</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./00-images/01-abeille.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">The MASSIF project</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./objectifs.html" rel="" target="">
 <span class="menu-text">Objectives</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./lt.html" rel="" target="" aria-current="page">
 <span class="menu-text">Work packages</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./consortium.html" rel="" target="">
 <span class="menu-text">Consortium</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./ressources.html" rel="" target="">
 <span class="menu-text">Ressources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://massif-project.github.io/site/" rel="" target="_blank">
 <span class="menu-text">🇫🇷 Français</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#work-packages" id="toc-work-packages" class="nav-link active" data-scroll-target="#work-packages">Work packages</a>
  <ul class="collapse">
  <li><a href="#wp0---project-management-and-dissemination-of-results" id="toc-wp0---project-management-and-dissemination-of-results" class="nav-link" data-scroll-target="#wp0---project-management-and-dissemination-of-results">WP0 - Project management and dissemination of results</a></li>
  <li><a href="#wp1---developing-reference-databases-to-build-a-reliable-identification-toolkit" id="toc-wp1---developing-reference-databases-to-build-a-reliable-identification-toolkit" class="nav-link" data-scroll-target="#wp1---developing-reference-databases-to-build-a-reliable-identification-toolkit">WP1 - Developing reference databases to build a reliable identification toolkit</a>
  <ul class="collapse">
  <li><a href="#task-1.1.-database-for-molecular-identification-of-specimens-barcoding" id="toc-task-1.1.-database-for-molecular-identification-of-specimens-barcoding" class="nav-link" data-scroll-target="#task-1.1.-database-for-molecular-identification-of-specimens-barcoding">Task 1.1. Database for molecular identification of specimens (barcoding)</a></li>
  <li><a href="#task-1.2.-development-of-a-photograph-reference-library" id="toc-task-1.2.-development-of-a-photograph-reference-library" class="nav-link" data-scroll-target="#task-1.2.-development-of-a-photograph-reference-library">Task 1.2. Development of a photograph reference library</a></li>
  </ul></li>
  <li><a href="#wp2---workflow-development-and-training-ai-algorithms-for-lab--and-field-based-insect-identification" id="toc-wp2---workflow-development-and-training-ai-algorithms-for-lab--and-field-based-insect-identification" class="nav-link" data-scroll-target="#wp2---workflow-development-and-training-ai-algorithms-for-lab--and-field-based-insect-identification">WP2 - Workflow development and training AI algorithms for lab- and field-based insect identification</a>
  <ul class="collapse">
  <li><a href="#task-2.1.-automating-insect-sorting-and-identification-in-the-lab" id="toc-task-2.1.-automating-insect-sorting-and-identification-in-the-lab" class="nav-link" data-scroll-target="#task-2.1.-automating-insect-sorting-and-identification-in-the-lab">Task 2.1. Automating insect sorting and identification in the lab</a></li>
  <li><a href="#task-2.2.-embedded-vision-models-for-non-invasive-identification-of-live-insects-trapped-in-the-field" id="toc-task-2.2.-embedded-vision-models-for-non-invasive-identification-of-live-insects-trapped-in-the-field" class="nav-link" data-scroll-target="#task-2.2.-embedded-vision-models-for-non-invasive-identification-of-live-insects-trapped-in-the-field">Task 2.2. Embedded vision models for non-invasive identification of live insects trapped in the field</a></li>
  </ul></li>
  <li><a href="#wp3---optimisation-of-trapping-strategies-and-development-of-sensor-based-instruments" id="toc-wp3---optimisation-of-trapping-strategies-and-development-of-sensor-based-instruments" class="nav-link" data-scroll-target="#wp3---optimisation-of-trapping-strategies-and-development-of-sensor-based-instruments">WP3 - Optimisation of trapping strategies and development of sensor-based instruments</a>
  <ul class="collapse">
  <li><a href="#task-3.1.-selection-and-optimization-of-trapping-systems-and-attractants-for-long-term-monitoring" id="toc-task-3.1.-selection-and-optimization-of-trapping-systems-and-attractants-for-long-term-monitoring" class="nav-link" data-scroll-target="#task-3.1.-selection-and-optimization-of-trapping-systems-and-attractants-for-long-term-monitoring">Task 3.1. Selection and optimization of trapping systems and attractants for long term monitoring</a></li>
  <li><a href="#task-3.2.-smart-embedded-image-analyser-system-for-long-term-insect-monitoring" id="toc-task-3.2.-smart-embedded-image-analyser-system-for-long-term-insect-monitoring" class="nav-link" data-scroll-target="#task-3.2.-smart-embedded-image-analyser-system-for-long-term-insect-monitoring">Task 3.2. Smart embedded image analyser system for long term insect monitoring</a></li>
  </ul></li>
  <li><a href="#wp4---design-and-implementation-of-sampling-strategy-and-analytical-workflow" id="toc-wp4---design-and-implementation-of-sampling-strategy-and-analytical-workflow" class="nav-link" data-scroll-target="#wp4---design-and-implementation-of-sampling-strategy-and-analytical-workflow">WP4 - Design and implementation of sampling strategy and analytical workflow</a>
  <ul class="collapse">
  <li><a href="#task-4.1.-definition-and-implementation-of-a-field-sampling-plan-for-pilot-sites" id="toc-task-4.1.-definition-and-implementation-of-a-field-sampling-plan-for-pilot-sites" class="nav-link" data-scroll-target="#task-4.1.-definition-and-implementation-of-a-field-sampling-plan-for-pilot-sites">Task 4.1. Definition and implementation of a field sampling plan for pilot sites</a></li>
  <li><a href="#task-4.2.-definition-and-implementation-of-a-pipeline-of-data-analysis-selection-of-metrics-and-calculation-methods" id="toc-task-4.2.-definition-and-implementation-of-a-pipeline-of-data-analysis-selection-of-metrics-and-calculation-methods" class="nav-link" data-scroll-target="#task-4.2.-definition-and-implementation-of-a-pipeline-of-data-analysis-selection-of-metrics-and-calculation-methods">Task 4.2. Definition and implementation of a pipeline of data analysis: selection of metrics and calculation methods</a></li>
  <li><a href="#task-4.3.-designing-the-large-scale-monitoring-strategy" id="toc-task-4.3.-designing-the-large-scale-monitoring-strategy" class="nav-link" data-scroll-target="#task-4.3.-designing-the-large-scale-monitoring-strategy">Task 4.3. Designing the large scale monitoring strategy</a></li>
  </ul></li>
  <li><a href="#wp5---data-management-and-visualization" id="toc-wp5---data-management-and-visualization" class="nav-link" data-scroll-target="#wp5---data-management-and-visualization">WP5 - Data management and visualization</a>
  <ul class="collapse">
  <li><a href="#task-5.1.-setup-and-tuning-of-an-institutional-infrastructure-to-store-secure-and-query-databases" id="toc-task-5.1.-setup-and-tuning-of-an-institutional-infrastructure-to-store-secure-and-query-databases" class="nav-link" data-scroll-target="#task-5.1.-setup-and-tuning-of-an-institutional-infrastructure-to-store-secure-and-query-databases">Task 5.1. Setup and tuning of an institutional infrastructure to store, secure and query databases</a></li>
  <li><a href="#task-5.2.-development-of-a-public-web-tool-to-explore-data-and-use-wp4-r-package" id="toc-task-5.2.-development-of-a-public-web-tool-to-explore-data-and-use-wp4-r-package" class="nav-link" data-scroll-target="#task-5.2.-development-of-a-public-web-tool-to-explore-data-and-use-wp4-r-package">Task 5.2. Development of a public web tool to explore data and use WP4 R package</a></li>
  <li><a href="#task-5.3.-creation-of-free-and-open-source-configurations-and-documentation-for-replication-and-migration-to-trans-institutional-infrastructures" id="toc-task-5.3.-creation-of-free-and-open-source-configurations-and-documentation-for-replication-and-migration-to-trans-institutional-infrastructures" class="nav-link" data-scroll-target="#task-5.3.-creation-of-free-and-open-source-configurations-and-documentation-for-replication-and-migration-to-trans-institutional-infrastructures">Task 5.3. Creation of free and open-source configurations and documentation for replication and migration to trans-institutional infrastructures</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Work packages</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="work-packages" class="level1">
<h1>Work packages</h1>
<div class="figure text-center">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="00-images/00-lts.png" class="text-center img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">MASSIF project organization chart</figcaption>
</figure>
</div>
</div>
<section id="wp0---project-management-and-dissemination-of-results" class="level2">
<h2 class="anchored" data-anchor-id="wp0---project-management-and-dissemination-of-results">WP0 - Project management and dissemination of results</h2>
<p>Coord.: Carole Kerdelhué (CBGP)</p>
</section>
<section id="wp1---developing-reference-databases-to-build-a-reliable-identification-toolkit" class="level2">
<h2 class="anchored" data-anchor-id="wp1---developing-reference-databases-to-build-a-reliable-identification-toolkit">WP1 - Developing reference databases to build a reliable identification toolkit</h2>
<p>Coord.: Jean-Claude Streito (CBGP) &amp; Mélodie Ollivier (DYNAFOR)</p>
<p><strong>Objectives:</strong> develop shared resources necessary for reliable insect identification. Two databases will be generated: Task 1.1 - a DNA sequence database (barcodes) validated by taxonomists and compatible with the Barcode of Life international database (BOLD); and Task 1.2 - an annotated image database to train artificial intelligence (AI) tools. The consortium has extensive expertise in taxonomic characterization and has been developing databases for over 10 years. Non-destructive extraction and high-quality photographs of specimens will preserve samples while ensuring reliable identification. Links with other WPs: WP1 will provide the necessary resources to WP2 for training and evaluation of AI algorithms, and WP3 &amp; 4 for eDNA and individual-based barcoding identification of field-trapped specimens. Databases will be developed with WP5.</p>
<section id="task-1.1.-database-for-molecular-identification-of-specimens-barcoding" class="level3">
<h3 class="anchored" data-anchor-id="task-1.1.-database-for-molecular-identification-of-specimens-barcoding">Task 1.1. Database for molecular identification of specimens (barcoding)</h3>
<p>We will first assess the current state of existing datasets and available specimens. Then molecular barcoding of non-included species will be generated either by Sanger- or high-throughput MiSeq sequencing, which is well mastered by the different partners. During the course of the project, the consortium will benefit from on-going developments of new technologies in the molecular biology platform of CBGP (ultrafast PCR and MinIon sequencing), which will allow massive production of individual barcodes (automation of processes and sequence analysis). The barcode reference database produced will enable 1- the identification of the Coleoptera specimens collected with conventional traps (WP3 &amp; WP4) and 2- the development of non-lethal monitoring approach based on eDNA for pollinators (WP3 &amp; WP4). This task involves the following steps: 1) Assessment of existing data and identification of gaps, 2) Collection of new specimens from institutional collections or field campaigns, 3) Enrichment of specimen metadata, 4) DNA sequencing processes (CO1 and 16S markers), bioinformatics analyses, and barcode validation, and 5) sequence publication (as a dataset with a doi) on BOLD and <a href="https://doi.org/10.15454/TBGRIB" target="“_blank”">Arthemis</a>. For maintenance and traceability, vouchers will be preserved at UMR CBGP <a href="https://doi.org/10.15454/D6XAKL" target="“_blank”">collection</a>.</p>
</section>
<section id="task-1.2.-development-of-a-photograph-reference-library" class="level3">
<h3 class="anchored" data-anchor-id="task-1.2.-development-of-a-photograph-reference-library">Task 1.2. Development of a photograph reference library</h3>
<p>The objective of this task is twofold: 1- produce high-quality photographs using the Keyence VHX5000 of reference specimens in the collections and voucher specimens of barcoded individuals related to task 1.1; and 2- produce between 100 and 1000 photographs of each species for training AI algorithms (WP2). For this second objective, we will gather the photographs available from all partners in the beginning of the project and expert-validated photographs available from open citizen science programs or naturalist social networks (SpiPoll, iNaturalist, INPN Espèces…). The database will be gradually supplemented by acquiring ad hoc lower-quality photographs, firstly using specimens identified by experts already available in the consortium collections and secondly, using specimens collected and identified as part of the project. A semi-automated imaging protocol using Keyence was already successfully tested by EFNO and will be finalized and used. Finally, photographs taken by the prototypes developed in WP3 directly in the field and annotated by expert taxonomists will enrich the database during the course of the project. Ultimately, this database will be made available in a public repository.</p>
</section>
</section>
<section id="wp2---workflow-development-and-training-ai-algorithms-for-lab--and-field-based-insect-identification" class="level2">
<h2 class="anchored" data-anchor-id="wp2---workflow-development-and-training-ai-algorithms-for-lab--and-field-based-insect-identification">WP2 - Workflow development and training AI algorithms for lab- and field-based insect identification</h2>
<p>Coord.: Kevin Darras (EFNO) &amp; Blaise Mulliez (LAAS)</p>
<p><strong>Objectives:</strong> automate and accelerate insect specimen identification, both in lab conditions and in natura. The ultimate goal is to rely primarily on non-invasive field monitoring techniques, complemented by occasional sample collection for specific taxa. The models will be used for (i) digital binocular microscope imagery that will automate the taxonomic identification for untrained personnel in lab conditions, and (ii) as embedded image analyzer system (WP3) to automatically sort and transmit only pictures containing target groups. This WP builds on the expertise of EFNO, SETE and LAAS with the photographic databases, object detection and image classification models, and embedded vision cameras. Links with other WPs: WP2 will use the resources developed in WP1 and analyze the data obtained in WP3 and WP4.</p>
<section id="task-2.1.-automating-insect-sorting-and-identification-in-the-lab" class="level3">
<h3 class="anchored" data-anchor-id="task-2.1.-automating-insect-sorting-and-identification-in-the-lab">Task 2.1. Automating insect sorting and identification in the lab</h3>
<p>Step 1, we will train object detection models to sort sampled specimens to order- or family-level. Initially, we will evaluate the use of photographs of identified specimens to artificially create images of insects scattered in trays and train the AI model. Step 2, we will develop AI models to reach species-level identification of our focal groups based on image classification models (e.g., ResNET50 or EfficientNet). Again, untrained personnel could thereby perform species-level identifications of insect specimens and put aside low-confidence classifications or false positives for targeted identification by taxonomic experts. Model evaluation will be carried out using new field samples, which can iteratively complement the training dataset until high-accuracy (mAP &gt; 0.8) models are obtained.</p>
</section>
<section id="task-2.2.-embedded-vision-models-for-non-invasive-identification-of-live-insects-trapped-in-the-field" class="level3">
<h3 class="anchored" data-anchor-id="task-2.2.-embedded-vision-models-for-non-invasive-identification-of-live-insects-trapped-in-the-field">Task 2.2. Embedded vision models for non-invasive identification of live insects trapped in the field</h3>
<p>We will train object detection models for Raspberry-Pi-based cameras and low-power openMV cameras. Here, the challenge lies in the great variability of the object sizes (from 2 mm to 5 cm). Region-of-interest detection and classification approaches will be compared against object detection (FOMO for openMV, YOLO 11 “nano” for Raspberry cameras) algorithms - optionally generalist ones combined with image classification - in terms of execution speed, power consumption, and workflow accuracy, while open-set AI models could be evaluated too (depending on technological readiness). Target invasive species or major pests might be detected directly and the result transmitted wirelessly. The depth of the embedded image classification will depend on the necessity to transmit results in real-time (i.e., for pest alerts), the power consumption of the transmission, and the data storage constraints. In relation with the capture chambers developed in WP3, limited image resolution will challenge identification depth for the smallest taxa. We will integrate optical magnifying lenses to gather close-up images and evaluate the usefulness of placing mirrors to gather lateral views of insects, which may also facilitate specimen identifications. Combining different imaging scales and angles requires designing tracking workflows to assign different images to the same individual and combine their identification likelihoods.</p>
</section>
</section>
<section id="wp3---optimisation-of-trapping-strategies-and-development-of-sensor-based-instruments" class="level2">
<h2 class="anchored" data-anchor-id="wp3---optimisation-of-trapping-strategies-and-development-of-sensor-based-instruments">WP3 - Optimisation of trapping strategies and development of sensor-based instruments</h2>
<p>Coord.: Adam Quotb (LAAS) &amp; Emilie Andrieu (DYNAFOR)</p>
<p><strong>Objectives:</strong> develop new tools to monitor insect communities in a semi-automated way. We will develop a multi-sensor and multi-trap system enabling standardized, semi-automated monitoring of insect diversity and phenology while keeping a low carbon footprint (Embedded AI, remote data collection). The objective is to enhance multi-taxa insect detection, identification and counting in the field based on the deep learning methods developed in WP2 and a multi-sensor strategy (weather, geolocation, images, and optoelectronics). Links with other WPs: WP3 will use the AI models developed in WP2, and will provide the prototypes of the multi-sensor traps deployed in WP4.</p>
<section id="task-3.1.-selection-and-optimization-of-trapping-systems-and-attractants-for-long-term-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="task-3.1.-selection-and-optimization-of-trapping-systems-and-attractants-for-long-term-monitoring">Task 3.1. Selection and optimization of trapping systems and attractants for long term monitoring</h3>
<p>The choice of the trapping system depends on flight characteristics of the target species, and the traps’ ability to guide live individuals through the capture chamber located in the smart embedded image analyzer system developed in Task 3.2. Based on preliminary results, we selected black and green Lindgren type traps for coleoptera species, and malaise type traps for pollinators. Both trapping systems will be adapted to forest ecosystem constraints: size and robustness (large mammal presence, understorey), location (on the ground or hanging from branches) and easy to set up by non-expert staff. To increase the target insect captures, two types of attractants (visual and olfactory) will be tested in the field during the first year (effectiveness and ease of use for both types of trap). Once in the trap (fall by gravity for coleoptera and oriented flight for pollinators), insects will have to pass through a capture chamber to escape. This guidance system will be developed and adapted to each trap type to allow passage through the laser (task 3.2) and in front of the optical lens, thereby ensuring the best possible image capture. In the prototype version of the trap, a dedicated system will be added to coleoptera traps to collect the specimens and to assess the quality of automatic AI-based identification by molecular barcoding and taxonomic expertise. The collection system of DNA traces (found in hairs, etc) will be added to the exit chamber for pollinators to differentiate cryptic species through eDNA analyses.</p>
</section>
<section id="task-3.2.-smart-embedded-image-analyser-system-for-long-term-insect-monitoring" class="level3">
<h3 class="anchored" data-anchor-id="task-3.2.-smart-embedded-image-analyser-system-for-long-term-insect-monitoring">Task 3.2. Smart embedded image analyser system for long term insect monitoring</h3>
<p>This smart embedded image analyzer consists of an electronic board enabling image acquisition (such as a Raspberry Pi or openMV) and onboard processing of the captured images by executing AI models developed in WP2. It includes external peripherals for capturing images as well as climatic conditions (temperature, humidity, etc), LEDs for illuminating the field of view, a laser sensor for insect passage detection and camera triggering, a local communication system (Wi-Fi + GSM), and mechanical adaptations for interfacing with traps developed in task 3.1. Raspberry-based devices will be developed starting from the Entomoscope device used in SETE, and openMV-based devices will be designed based on the ecoEye device which continues to be developed at EFNO. Raspberry mini-computers and openMV boards can execute region-of-interest detection natively using frame differencing techniques. Raspberries can execute small-scale object detection models (YOLO nano) and OpenMV devices, which have no operating system, require special libraries to execute AI models based on TensorFlow Lite (openMV libtflm) or MobileNet V2-based FOMO models. In addition, batteries coupled to a solar charging system will provide the necessary power supply to the system in the long run. During the 18 first months of the project, task 3.1 and 3.2 will be carried out in parallel. At the end of this period, 9 prototypes will be delivered to the consortium members for tests in real conditions in URZF, EFNO and DYNAFOR. Once the initial prototypes have been validated, a second prototype version for large scale monitoring (100 - 150 units) will be developed by the LAAS and EFNO and delivered to the consortium to be deployed in the pilot sites (WP4).</p>
</section>
</section>
<section id="wp4---design-and-implementation-of-sampling-strategy-and-analytical-workflow" class="level2">
<h2 class="anchored" data-anchor-id="wp4---design-and-implementation-of-sampling-strategy-and-analytical-workflow">WP4 - Design and implementation of sampling strategy and analytical workflow</h2>
<p>Coord.: C. Bouget (EFNO) &amp; J.-P. Rossi (CBGP)</p>
<p><strong>Objectives:</strong> 1) test the multi-sensor traps and the insect identification tools in forest field conditions, 2) propose a set of statistical tools (made available through R package) for analyzing insect communities sampled with automatic traps, and 3) support decision-making in designing a large-scale national monitoring strategy. Links with other WPs: WP4 will use the tools and resources developed in WP1, WP2 and WP3.</p>
<section id="task-4.1.-definition-and-implementation-of-a-field-sampling-plan-for-pilot-sites" class="level3">
<h3 class="anchored" data-anchor-id="task-4.1.-definition-and-implementation-of-a-field-sampling-plan-for-pilot-sites">Task 4.1. Definition and implementation of a field sampling plan for pilot sites</h3>
<p>First, we will compile an inventory of available datasets (xylophagous and floricolous communities) from the consortium’s laboratories. These datasets will be used to test the performance of different predictive sampling plans before original data are produced in the project. Once the prototypes of automatic traps developed in WP3 are available, we will select 10 pilot sites where intensive sampling will be carried out to quantify the impact of varying sampling effort on biodiversity metrics (4.2) to help design the final, large-scale sampling strategy proposed in 4.3. These 10 sites will reflect the diversity of French forest conditions, including 1- biological reserves from the National Forest Office’s monitoring network (Fontainebleau, Haut Tuileau, Ecouves, Vallon du Maupas, Ste Baume…), 2- living labs promoted by the FORESTT PEPR (Chantilly, Compiègne, Landes, Briançonnais, Pyrénées ariégeoises), 3- Renecofor plots (Fontainebleau) to align with the FP MONITOR design, and 4- long-term monitoring sites managed by project partners (ZA PyGar). 150 automatic traps (50 green multi-funnel traps, 50 black multi-funnel traps and 50 pollinator traps) will be deployed across these 10 sites, with 5 traps of each type per plot. The species lists generated by three methods (visual expert identification, DNA barcoding, AI-based image analysis) will be compared to assess the performance of the prototype automatic photo-trap. The differences between methods will be analyzed at varying taxonomic resolutions (from specific to supra-specific levels).</p>
</section>
<section id="task-4.2.-definition-and-implementation-of-a-pipeline-of-data-analysis-selection-of-metrics-and-calculation-methods" class="level3">
<h3 class="anchored" data-anchor-id="task-4.2.-definition-and-implementation-of-a-pipeline-of-data-analysis-selection-of-metrics-and-calculation-methods">Task 4.2. Definition and implementation of a pipeline of data analysis: selection of metrics and calculation methods</h3>
<p>We will identify key community metrics to be estimated, including species richness, Hill numbers, functional diversity indices based on available functional traits databases, and eDNA-specific diversity metrics ; and specific indicators focusing either on invasive species or emergent forest pests (risk indicators), or on conservation species (conservation indicators). We will develop statistical methods to compute these metrics and assess their confidence intervals and bias using randomization approaches such as bootstrapping. They will be implemented as an R package hosted on GitHub. These tools will be used to analyze the datasets from Task 4.1 and simulate the impact of decreasing the sampling effort per site upon the metrics and their uncertainty (rarefaction). The R package will integrate with the Shiny application developed in WP5. We will also evaluate the impact of potential limitations in taxonomic-level resolution obtained from AI algorithms.</p>
</section>
<section id="task-4.3.-designing-the-large-scale-monitoring-strategy" class="level3">
<h3 class="anchored" data-anchor-id="task-4.3.-designing-the-large-scale-monitoring-strategy">Task 4.3. Designing the large scale monitoring strategy</h3>
<p>The automatic sensor should be deployable across a national network of several dozen sites, requiring minimal and infrequent human intervention. Using results obtained in Task 4.2, we will define scenarii based on progressively increasing sampling efforts per site to assess biodiversity metrics, detect rare or key species, and monitor invasive or emerging pests. We will propose two large-scale sampling strategies related to contrasted monitoring objectives: (i) a systematic network of large-scale forest plots (e.g., Renecofor), already multi-instrumented through the FP MONITOR passive acoustic monitoring program, and (ii) a network of sites along ecological gradients, such as harvested versus reserve plots, to evaluate the impact of land-sparing strategies on biodiversity conservation.</p>
</section>
</section>
<section id="wp5---data-management-and-visualization" class="level2">
<h2 class="anchored" data-anchor-id="wp5---data-management-and-visualization">WP5 - Data management and visualization</h2>
<p>Coord.: Mathieu Laparie (URZF) &amp; Wilfried Heintz (DYNAFOR); FP NUM-DATA members as external advisors.</p>
<p><strong>Objectives:</strong> develop a reproducible software strategy deployed on an institutional server, ensuring its portability to other infrastructures. Automated data acquisition systems and AI-based species recognition generate large volumes of data, requiring a robust software architecture for database management designed to (i) ensure long-term maintenance and administration and (ii) allow scalability as the device network expands and data volumes increase. Links with other WPs: WP5 is transversal and thus related to all other WPs. The architecture will be optimized to efficiently manage and secure the data produced by MASSIF smart devices (5.1) while guaranteeing the system’s long-term sustainability (5.3). A web service for authenticated project members will be implemented, enabling secure querying and retrieval of data. Furthermore, a public R Shiny application will be developed to provide a user-friendly interface to the functionalities of the R package developed in WP4 (5.2). The container will first be set up on isolated test machines in collaboration with an experienced system administrator to make informed decisions about critical aspects such as database interoperability, security configurations, or containerization solutions for scalability and reproducibility. Ongoing consultations will take place with external experts, including a DevOps/Software Engineer and three NUM-DATA information systems specialists.</p>
<section id="task-5.1.-setup-and-tuning-of-an-institutional-infrastructure-to-store-secure-and-query-databases" class="level3">
<h3 class="anchored" data-anchor-id="task-5.1.-setup-and-tuning-of-an-institutional-infrastructure-to-store-secure-and-query-databases">Task 5.1. Setup and tuning of an institutional infrastructure to store, secure and query databases</h3>
<p>A virtual machine will be deployed on a public research institution server to centralize and host data from prototype sensors and AI models. Field (WP4) and reference (WP1) data will be stored in optimized database formats (e.g., PostgreSQL/PostGIS for spatial data, SQLite for lightweight needs) to ensure interoperability and query efficiency. To promote open science and replicability, we will prioritize Free and Open Source Software (FOSS), use Git versioning, and provide public documentation with default configurations and scripts. Containerization (e.g., Docker or Kubernetes) will decouple the development environment from hardware, facilitating seamless migration, redundancy, and cost-effective long-term administration across research groups.</p>
</section>
<section id="task-5.2.-development-of-a-public-web-tool-to-explore-data-and-use-wp4-r-package" class="level3">
<h3 class="anchored" data-anchor-id="task-5.2.-development-of-a-public-web-tool-to-explore-data-and-use-wp4-r-package">Task 5.2. Development of a public web tool to explore data and use WP4 R package</h3>
<p>We will implement a web authentication system to enable project members to access and annotate the hosted data, and develop a public R Shiny application to allow users to dynamically interact with biodiversity data. Users will upload monitoring datasets and compute biodiversity metrics and associated statistics using the R package from WP4. Initially hosted on the project’s virtual machine for testing, the tool will ultimately be deployed on an institutional server managed by dedicated <a href="https://sk8.inrae.fr/00-leprojetsk8.html" target="“_blank”">INRAE teams</a> to ensure long-term reliability and maintenance beyond the project’s duration.</p>
</section>
<section id="task-5.3.-creation-of-free-and-open-source-configurations-and-documentation-for-replication-and-migration-to-trans-institutional-infrastructures" class="level3">
<h3 class="anchored" data-anchor-id="task-5.3.-creation-of-free-and-open-source-configurations-and-documentation-for-replication-and-migration-to-trans-institutional-infrastructures">Task 5.3. Creation of free and open-source configurations and documentation for replication and migration to trans-institutional infrastructures</h3>
<p>Comprehensive configurations and documentation will be developed throughout the project to enable replication and migration to various infrastructures or self-hosted solutions. This will include 1- reproducible configurations (using Docker or Kubernetes-based containerization), 2- open-source, version-controlled documentation for standardizing redundant or self-hosted server setups, 3- guidelines for scaling data flows and migrating to larger, supra-institutional data centers, reducing dependency on specific partners. Continuous support from NUM-DATA experts will ensure that technical decisions remain compatible with databases from other national or international biodiversity monitoring initiatives.</p>
<p><br><br></p>
<hr>
<p>Last update: September 12 2025</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>